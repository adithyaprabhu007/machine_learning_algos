{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMJCRSSD8/5Ju4a3MJhxqLz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adithyaprabhu007/machine_learning_algos/blob/main/iris_logistic_regression_scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# --- Load and Prepare Iris Dataset ---\n",
        "iris = load_iris()\n",
        "# Select data for two classes (e.g., Versicolor and Virginica)\n",
        "X_iris = iris.data[50:150, [2, 3]] # Using petal length and petal width\n",
        "y_iris = iris.target[50:150]\n",
        "\n",
        "# Convert labels to 0 and 1\n",
        "# Versicolor (1) becomes 0, Virginica (2) becomes 1\n",
        "y_iris = (y_iris - 1).astype(int)\n",
        "\n",
        "\n",
        "# Separate features (X) and labels (y)\n",
        "# Reshape X to get shape (num_features, num_samples)\n",
        "X = X_iris.T\n",
        "# Reshape y to get shape (1, num_samples)\n",
        "y = y_iris.reshape(1, -1)\n",
        "\n",
        "\n",
        "# --- 2. Core Mathematical Functions ---\n",
        "\n",
        "def sigmoid(z):\n",
        "    \"\"\"\n",
        "    Computes the sigmoid activation function.\n",
        "    \"\"\"\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "def compute_cost(y_hat, y):\n",
        "    \"\"\"\n",
        "    Computes the binary cross-entropy cost.\n",
        "    \"\"\"\n",
        "    m = y.shape[1] # Number of samples is the second dimension after reshaping\n",
        "    # To prevent log(0) errors\n",
        "    epsilon = 1e-15\n",
        "    cost = -1/m * np.sum(y * np.log(y_hat + epsilon) + (1 - y) * np.log(1 - y_hat + epsilon))\n",
        "    return np.squeeze(cost)\n",
        "\n",
        "# --- 3. The Training Function (Gradient Descent) ---\n",
        "\n",
        "def train_model(X, y, learning_rate=0.03, num_epochs=2000):\n",
        "    \"\"\"\n",
        "    Trains the logistic regression model using gradient descent.\n",
        "    \"\"\"\n",
        "    num_features = X.shape[0]\n",
        "    num_samples = X.shape[1]\n",
        "\n",
        "    # Initialize weights and bias\n",
        "    W = np.zeros((num_features, 1))\n",
        "    b = 0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # Forward pass\n",
        "        Z = np.dot(W.T, X) + b\n",
        "        y_hat = sigmoid(Z)\n",
        "\n",
        "        # Compute cost (for monitoring)\n",
        "        # y_hat and y should both have shape (1, num_samples)\n",
        "        cost = compute_cost(y_hat, y)\n",
        "\n",
        "        # Backward pass (gradient calculation)\n",
        "        dZ = y_hat - y\n",
        "        dW = (1 / num_samples) * np.dot(X, dZ.T)\n",
        "        db = (1 / num_samples) * np.sum(dZ)\n",
        "\n",
        "        learning_rate_new = learning_rate+\n",
        "        # Update parameters\n",
        "        W -= learning_rate_new * dW\n",
        "        b -= learning_rate_new * db\n",
        "\n",
        "        if epoch % 100 == 0:\n",
        "            print(f\"Epoch {epoch}: Cost = {cost:.4f}\")\n",
        "\n",
        "    return W, b\n",
        "\n",
        "# --- 4. The Prediction Function ---\n",
        "\n",
        "def predict(X, W, b):\n",
        "    \"\"\"\n",
        "    Makes predictions based on the trained model.\n",
        "    \"\"\"\n",
        "    Z = np.dot(W.T, X) + b\n",
        "    y_hat = sigmoid(Z)\n",
        "    y_prediction = (y_hat > 0.5).astype(int)\n",
        "    return y_prediction\n",
        "\n",
        "# --- Main execution block ---\n",
        "if __name__ == \"__main__\":\n",
        "    # Train the model\n",
        "    W, b = train_model(X, y, learning_rate=0.06, num_epochs=3600)\n",
        "\n",
        "    # Make predictions on the training data\n",
        "    y_prediction = predict(X, W, b)\n",
        "\n",
        "    # Calculate and print the accuracy\n",
        "    # y_prediction and y should both have shape (1, num_samples) for direct comparison\n",
        "    accuracy = np.mean(y_prediction == y) * 100\n",
        "    print(f\"\\nModel Accuracy: {accuracy:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7am_jSv79_yo",
        "outputId": "b6e3d684-aac6-4e1d-e220-4dbfd222ec4a"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0: Cost = 0.6931\n",
            "Epoch 100: Cost = 0.6305\n",
            "Epoch 200: Cost = 0.5718\n",
            "Epoch 300: Cost = 0.5150\n",
            "Epoch 400: Cost = 0.4640\n",
            "Epoch 500: Cost = 0.4200\n",
            "Epoch 600: Cost = 0.3829\n",
            "Epoch 700: Cost = 0.3518\n",
            "Epoch 800: Cost = 0.3255\n",
            "Epoch 900: Cost = 0.3034\n",
            "Epoch 1000: Cost = 0.2844\n",
            "Epoch 1100: Cost = 0.2681\n",
            "Epoch 1200: Cost = 0.2540\n",
            "Epoch 1300: Cost = 0.2416\n",
            "Epoch 1400: Cost = 0.2308\n",
            "Epoch 1500: Cost = 0.2211\n",
            "Epoch 1600: Cost = 0.2125\n",
            "Epoch 1700: Cost = 0.2048\n",
            "Epoch 1800: Cost = 0.1979\n",
            "Epoch 1900: Cost = 0.1917\n",
            "Epoch 2000: Cost = 0.1860\n",
            "Epoch 2100: Cost = 0.1808\n",
            "Epoch 2200: Cost = 0.1760\n",
            "Epoch 2300: Cost = 0.1717\n",
            "Epoch 2400: Cost = 0.1677\n",
            "Epoch 2500: Cost = 0.1640\n",
            "Epoch 2600: Cost = 0.1606\n",
            "Epoch 2700: Cost = 0.1574\n",
            "Epoch 2800: Cost = 0.1545\n",
            "Epoch 2900: Cost = 0.1518\n",
            "Epoch 3000: Cost = 0.1492\n",
            "Epoch 3100: Cost = 0.1469\n",
            "Epoch 3200: Cost = 0.1446\n",
            "Epoch 3300: Cost = 0.1426\n",
            "Epoch 3400: Cost = 0.1406\n",
            "Epoch 3500: Cost = 0.1388\n",
            "\n",
            "Model Accuracy: 93.00%\n"
          ]
        }
      ]
    }
  ]
}